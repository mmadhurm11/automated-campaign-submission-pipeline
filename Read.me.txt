# Automated Campaign Submission Filtering 

### ğŸ“Œ Problem
Student campaigns often faced **fake engagement**:
- Low-effort responses like "ok", "done", "nice".
- Multiple submissions from the same IP to meet minimum criteria.
- Manual filtering wasted time but still missed spam.
- This led to **low ROI** and fewer genuine active users.

---

### ğŸ¯ Solution
I built a **2-stage automation pipeline** using **Make.com, Google Sheets, and Slack**:

1. **Quality Check** â†’ filters spam/short comments, checks submission time â‰¥ 20 sec.  
2. **IP Uniqueness Check** â†’ allows max 2 submissions per IP, else flagged.  
3. **Slack Alerts** â†’ suspicious entries sent instantly to campaign team.  
4. **Clean Data Output** â†’ valid vs flagged submissions stored in separate Google Sheets.

---

### âš™ï¸ Workflow
![Pipeline Screenshot](pipeline/pipeline_screenshot.png)

- **Trigger:** Google Sheets (Watch new rows)  
- **Router 1:** Quality Check â†’ pass/fail â†’ store in Valid/Flagged sheets  
- **Router 2:** IP Check â†’ prevent duplicates  
- **Slack Integration:** Real-time alerts for flagged entries  

---

### ğŸ“Š Outcomes
- ğŸš« Flagged **35%+ fake/invalid entries** automatically  
- â³ Reduced manual review workload by **~80%**  
- âœ… Improved dataset **accuracy by 40%+**  
- ğŸ“ˆ Delivered **trustworthy ROI measurement** for campaigns  

---

### ğŸ“‚ Repository Contents
- `data/campaign_submissions.xlsx` â†’ Sample input dataset  
- `pipeline/grapevine_pipeline.json` â†’ Exported Make.com workflow  
- `pipeline/pipeline_screenshot.png` â†’ Visual of workflow  
- `docs/slide_deck.pdf` â†’ Project presentation  
- `links.txt` â†’ Link to the live Make.com pipeline  

---

### ğŸš€ Impact
This project showcases:
- **Automation for business ROI**  
- **Product-thinking approach** (cleaning user engagement signals)  
- **Scalable system design** using no-code tools  

---

### ğŸ”— Live Pipeline
ğŸ‘‰ [Click here to view the live Make.com pipeline](https://eu2.make.com/2709273/scenarios/7360894/edit)
